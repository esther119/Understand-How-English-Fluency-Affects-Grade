{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "\n",
    "# from bs4 import BeautifulSoup\n",
    "# import mpld3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/swimmingcircle/Documents/CS156/fccmediumTitles_Cleaned_Data.tsv', on_bad_lines='skip', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quartier</th>\n",
       "      <th>date</th>\n",
       "      <th>Title</th>\n",
       "      <th>Recommends</th>\n",
       "      <th>Read ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>October 2016</td>\n",
       "      <td>Upgrading to MacOS Sierra will break your SSH ...</td>\n",
       "      <td>58</td>\n",
       "      <td>48%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>October 2016</td>\n",
       "      <td>How Crowd Curation Improved Our Search Quality...</td>\n",
       "      <td>28</td>\n",
       "      <td>37%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>October 2016</td>\n",
       "      <td>Code Briefing: What I learned from reviewing 5...</td>\n",
       "      <td>56</td>\n",
       "      <td>59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>October 2016</td>\n",
       "      <td>What I learned from reviewing 50 portfolios on...</td>\n",
       "      <td>635</td>\n",
       "      <td>49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>October 2016</td>\n",
       "      <td>JavaScript Fatigue Fatigue In Free Code CampVi...</td>\n",
       "      <td>1100</td>\n",
       "      <td>55%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>1</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>Gulp! I Improved my Workflow! In Free Code Cam...</td>\n",
       "      <td>49</td>\n",
       "      <td>30%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>1</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>Beginners Guide to Big O Notation In Free Code...</td>\n",
       "      <td>108</td>\n",
       "      <td>54%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>1</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>7 Ways Streaming Makes you a Better Coder In F...</td>\n",
       "      <td>142</td>\n",
       "      <td>61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>1</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>Jump Start Your Local Campsite with Coffee-and...</td>\n",
       "      <td>50</td>\n",
       "      <td>70%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>1</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>Commit to Yourself. Commit to a Nonprofit. In ...</td>\n",
       "      <td>23</td>\n",
       "      <td>80%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>453 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Quartier          date  \\\n",
       "0           4  October 2016   \n",
       "1           4  October 2016   \n",
       "2           4  October 2016   \n",
       "3           4  October 2016   \n",
       "4           4  October 2016   \n",
       "..        ...           ...   \n",
       "448         1  October 2015   \n",
       "449         1  October 2015   \n",
       "450         1  October 2015   \n",
       "451         1  October 2015   \n",
       "452         1  October 2015   \n",
       "\n",
       "                                                 Title  Recommends Read ratio  \n",
       "0    Upgrading to MacOS Sierra will break your SSH ...          58        48%  \n",
       "1    How Crowd Curation Improved Our Search Quality...          28        37%  \n",
       "2    Code Briefing: What I learned from reviewing 5...          56        59%  \n",
       "3    What I learned from reviewing 50 portfolios on...         635        49%  \n",
       "4    JavaScript Fatigue Fatigue In Free Code CampVi...        1100        55%  \n",
       "..                                                 ...         ...        ...  \n",
       "448  Gulp! I Improved my Workflow! In Free Code Cam...          49        30%  \n",
       "449  Beginners Guide to Big O Notation In Free Code...         108        54%  \n",
       "450  7 Ways Streaming Makes you a Better Coder In F...         142        61%  \n",
       "451  Jump Start Your Local Campsite with Coffee-and...          50        70%  \n",
       "452  Commit to Yourself. Commit to a Nonprofit. In ...          23        80%  \n",
       "\n",
       "[453 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "# use nltk.download() to install the corpus first\n",
    "# Stop Words are words which do not contain important significance to be used in Search Queries\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "- Word tokenize: We use the word_tokenize() method to split a sentence into tokens or words. \n",
    "- Sentence tokenize: We use the sent_tokenize() method to split a document or paragraph into sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upgrading to MacOS Sierra will break your SSH keys and lock you out of your own servers.',\n",
       " 'In Free Code CampView storyReferrers']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = [sent for sent in nltk.sent_tokenize(df['Title'][0])]\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upgrading',\n",
       " 'to',\n",
       " 'MacOS',\n",
       " 'Sierra',\n",
       " 'will',\n",
       " 'break',\n",
       " 'your',\n",
       " 'SSH',\n",
       " 'keys',\n",
       " 'and',\n",
       " 'lock',\n",
       " 'you',\n",
       " 'out',\n",
       " 'of',\n",
       " 'your',\n",
       " 'own',\n",
       " 'servers',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in nltk.word_tokenize(sents[0])]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Upgrading',\n",
       " 'to',\n",
       " 'MacOS',\n",
       " 'Sierra',\n",
       " 'will',\n",
       " 'break',\n",
       " 'your',\n",
       " 'SSH',\n",
       " 'keys',\n",
       " 'and',\n",
       " 'lock',\n",
       " 'you',\n",
       " 'out',\n",
       " 'of',\n",
       " 'your',\n",
       " 'own',\n",
       " 'servers']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "filtered_words = []\n",
    "for word in words:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            filtered_words.append(word)\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stem: find the rooting of the word\n",
    "\n",
    "- Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upgrad',\n",
       " 'to',\n",
       " 'maco',\n",
       " 'sierra',\n",
       " 'will',\n",
       " 'break',\n",
       " 'your',\n",
       " 'ssh',\n",
       " 'key',\n",
       " 'and',\n",
       " 'lock',\n",
       " 'you',\n",
       " 'out',\n",
       " 'of',\n",
       " 'your',\n",
       " 'own',\n",
       " 'server']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how \"only\" is stemmed to \"onli\" and \"wedding\" is stemmed to \"wed\"\n",
    "stems = [stemmer.stem(t) for t in filtered_words]\n",
    "stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upgrad',\n",
       " 'to',\n",
       " 'maco',\n",
       " 'sierra',\n",
       " 'will',\n",
       " 'break',\n",
       " 'your',\n",
       " 'ssh',\n",
       " 'key',\n",
       " 'and',\n",
       " 'lock',\n",
       " 'you',\n",
       " 'out',\n",
       " 'of',\n",
       " 'your',\n",
       " 'own',\n",
       " 'server',\n",
       " 'in',\n",
       " 'free',\n",
       " 'code',\n",
       " 'campview',\n",
       " 'storyreferr']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_stem(df['Title'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in df['Title']:\n",
    "    allwords_stemmed = tokenize_and_stem(i) # for each item in 'synopses', tokenize/stem\n",
    "    totalvocab_stemmed.extend(allwords_stemmed) # extend the 'totalvocab_stemmed' list\n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5867\n",
      "5867\n"
     ]
    }
   ],
   "source": [
    "print(len(totalvocab_stemmed))\n",
    "print(len(totalvocab_tokenized))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 5867 items in vocab_frame\n",
      "            words\n",
      "upgrad  upgrading\n",
      "to             to\n",
      "maco        macos\n",
      "sierra     sierra\n",
      "will         will\n"
     ]
    }
   ],
   "source": [
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('there are ' + str(vocab_frame.shape[0]) + ' items in vocab_frame')\n",
    "print(vocab_frame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use tf-idf to find the common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_df: this is the maximum frequency within the documents a given feature can have to be used in the tfi-idf matrix. If the term is in greater than 80% of the documents it probably cares little meanining (in the context of film synopses)\n",
    "\n",
    "- min_idf: this could be an integer (e.g. 5) and the term would have to be in at least 5 of the documents to be considered. Here I pass 0.2; the term must be in at least 20% of the document. I found that if I allowed a lower min_df I ended up basing clustering on names--for example \"Michael\" or \"Tom\" are names found in several of the movies and the synopses use these names frequently, but the names carry no real meaning.\n",
    "\n",
    "- ngram_range: this just means I'll look at unigrams, bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(453, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swimmingcircle/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/swimmingcircle/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that the result of this block takes a while to show\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.05, stop_words='english',\n",
    "                                 use_idf=True, tokenizer= tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Title']) #fit the vectorizer to synopses\n",
    "\n",
    "# (100, 563) means the matrix has 100 rows and 563 columns\n",
    "print(tfidf_matrix.shape)\n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<453x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best', 'build', 'design', 'develop', 'javascript', 'learn', 's', 'use']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sents ['Upgrading to MacOS Sierra will break your SSH keys and lock you out of your own servers.', 'In Free Code CampView storyReferrers']\n",
      "Shape of words_matrix (2, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swimmingcircle/Library/Python/3.9/lib/python/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/swimmingcircle/Library/Python/3.9/lib/python/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# A short example using the sentences above\n",
    "words_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "words_matrix = words_vectorizer.fit_transform(sents) #fit the vectorizer to synopses\n",
    "\n",
    "tfidf_tokens = words_vectorizer.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data = words_matrix.toarray(),index = ['Sentence 1','Sentence 2'], columns = tfidf_tokens)\n",
    "\n",
    "\n",
    "print('Sents', sents)\n",
    "# print('Words_matrix', words_matrix)\n",
    "print('Shape of words_matrix', words_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>break</th>\n",
       "      <th>break ssh</th>\n",
       "      <th>break ssh key</th>\n",
       "      <th>campview</th>\n",
       "      <th>campview storyreferr</th>\n",
       "      <th>code</th>\n",
       "      <th>code campview</th>\n",
       "      <th>code campview storyreferr</th>\n",
       "      <th>free</th>\n",
       "      <th>free code</th>\n",
       "      <th>...</th>\n",
       "      <th>sierra</th>\n",
       "      <th>sierra break</th>\n",
       "      <th>sierra break ssh</th>\n",
       "      <th>ssh</th>\n",
       "      <th>ssh key</th>\n",
       "      <th>ssh key lock</th>\n",
       "      <th>storyreferr</th>\n",
       "      <th>upgrad</th>\n",
       "      <th>upgrad maco</th>\n",
       "      <th>upgrad maco sierra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence 1</th>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "      <td>0.218218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence 2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               break  break ssh  break ssh key  campview  \\\n",
       "Sentence 1  0.218218   0.218218       0.218218  0.000000   \n",
       "Sentence 2  0.000000   0.000000       0.000000  0.333333   \n",
       "\n",
       "            campview storyreferr      code  code campview  \\\n",
       "Sentence 1              0.000000  0.000000       0.000000   \n",
       "Sentence 2              0.333333  0.333333       0.333333   \n",
       "\n",
       "            code campview storyreferr      free  free code  ...    sierra  \\\n",
       "Sentence 1                   0.000000  0.000000   0.000000  ...  0.218218   \n",
       "Sentence 2                   0.333333  0.333333   0.333333  ...  0.000000   \n",
       "\n",
       "            sierra break  sierra break ssh       ssh   ssh key  ssh key lock  \\\n",
       "Sentence 1      0.218218          0.218218  0.218218  0.218218      0.218218   \n",
       "Sentence 2      0.000000          0.000000  0.000000  0.000000      0.000000   \n",
       "\n",
       "            storyreferr    upgrad  upgrad maco  upgrad maco sierra  \n",
       "Sentence 1     0.000000  0.218218     0.218218            0.218218  \n",
       "Sentence 2     0.333333  0.000000     0.000000            0.000000  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidfvect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_analyzer()` returns a callable that let's you extract the tokenizing step from the transformation pipeline wrapped in the CountVectorizer or TfidfVectorizer. You can do something like this:\n",
    "\n",
    "`analyze = vectorizer.build_analyzer()`\n",
    "\n",
    "`df['Text'].apply(lambda x: analyze(x)) #or df['Text'].apply(analyze)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how we get the 18 terms\n",
    "analyze = words_vectorizer.build_analyzer()\n",
    "# print(analyze(\"Today (May 19, 2016) is his only daughter's wedding.\"))\n",
    "# print(analyze(\"Vito Corleone is the Godfather.\"))\n",
    "# print(analyze(\"Vito's youngest son, Michael, in a Marine Corps uniform, introduces his girlfriend, Kay Adams, to his family at the sprawling reception.\"))\n",
    "# all_terms = words_vectorizer.get_feature_names()\n",
    "# print(all_terms)\n",
    "# print(len(all_terms))\n",
    "\n",
    "# sent 1 and 2, similarity 0, sent 1 and 3 shares \"his\", sent 2 and 3 shares Vito - try to change Vito's in sent3 to His and see the similary matrix changes\n",
    "example_similarity = cosine_similarity(words_matrix)\n",
    "example_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<453x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 263 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step: embedding the words \n",
    "- pretrained embedding: Glove \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
