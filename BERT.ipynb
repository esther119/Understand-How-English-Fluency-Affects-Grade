{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('vectorized_data.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polls ID</th>\n",
       "      <th>Poll Responses Response</th>\n",
       "      <th>Assessment reports Hashtag</th>\n",
       "      <th>Assessment reports Score</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>tokenized_responses</th>\n",
       "      <th>stemmed_responses</th>\n",
       "      <th>clean_responses</th>\n",
       "      <th>string</th>\n",
       "      <th>LOs/ HCs</th>\n",
       "      <th>...</th>\n",
       "      <th>vec_11</th>\n",
       "      <th>vec_12</th>\n",
       "      <th>vec_13</th>\n",
       "      <th>vec_14</th>\n",
       "      <th>vec_15</th>\n",
       "      <th>vec_16</th>\n",
       "      <th>vec_17</th>\n",
       "      <th>vec_18</th>\n",
       "      <th>vec_19</th>\n",
       "      <th>vec_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12522</td>\n",
       "      <td>The strengths of Plato's approach is his const...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['The', 'strengths', 'of', \"Plato's\", 'approac...</td>\n",
       "      <td>['the', 'strength', 'of', 'plato', 'approach',...</td>\n",
       "      <td>['strength', 'plato', 'approach', 'construct',...</td>\n",
       "      <td>strength plato approach construct whole framew...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227378</td>\n",
       "      <td>0.228062</td>\n",
       "      <td>-0.065646</td>\n",
       "      <td>-0.018259</td>\n",
       "      <td>0.589726</td>\n",
       "      <td>0.165162</td>\n",
       "      <td>0.243406</td>\n",
       "      <td>0.041697</td>\n",
       "      <td>0.047487</td>\n",
       "      <td>-0.080301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12522</td>\n",
       "      <td>In the breakout we discussed if outside the ca...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>['In', 'the', 'breakout', 'we', 'discussed', '...</td>\n",
       "      <td>['in', 'the', 'breakout', 'we', 'discuss', 'if...</td>\n",
       "      <td>['breakout', 'discuss', 'outsid', 'cave', 'big...</td>\n",
       "      <td>breakout discuss outsid cave bigger cave thus ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067890</td>\n",
       "      <td>-0.172406</td>\n",
       "      <td>-0.026694</td>\n",
       "      <td>-0.313533</td>\n",
       "      <td>0.335412</td>\n",
       "      <td>-0.184949</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>0.093128</td>\n",
       "      <td>0.022665</td>\n",
       "      <td>0.142223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12522</td>\n",
       "      <td>Back to cmmon confusion time: the section 'und...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Back', 'to', 'cmmon', 'confusion', 'time', '...</td>\n",
       "      <td>['back', 'to', 'cmmon', 'confus', 'time', 'the...</td>\n",
       "      <td>['back', 'cmmon', 'confus', 'time', 'section',...</td>\n",
       "      <td>back cmmon confus time section understand inte...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172736</td>\n",
       "      <td>0.221275</td>\n",
       "      <td>0.215809</td>\n",
       "      <td>-0.060970</td>\n",
       "      <td>0.372512</td>\n",
       "      <td>-0.060175</td>\n",
       "      <td>0.251572</td>\n",
       "      <td>0.366057</td>\n",
       "      <td>0.229787</td>\n",
       "      <td>0.318017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12522</td>\n",
       "      <td>Most difficult weakness is that his position w...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Most', 'difficult', 'weakness', 'is', 'that'...</td>\n",
       "      <td>['most', 'difficult', 'weak', 'is', 'that', 'h...</td>\n",
       "      <td>['difficult', 'weak', 'posit', 'understand', '...</td>\n",
       "      <td>difficult weak posit understand testabl like i...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134589</td>\n",
       "      <td>-0.047511</td>\n",
       "      <td>-0.054336</td>\n",
       "      <td>-0.222005</td>\n",
       "      <td>0.227974</td>\n",
       "      <td>-0.272034</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>-0.173518</td>\n",
       "      <td>0.228723</td>\n",
       "      <td>0.078310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12522</td>\n",
       "      <td>I'm still trying to understand the significanc...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I'm\", 'still', 'trying', 'to', 'understand',...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'to', 'understand', 't...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'understand', 'signifi...</td>\n",
       "      <td>i'm still tri understand signific cave analog ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025134</td>\n",
       "      <td>0.092645</td>\n",
       "      <td>-0.150749</td>\n",
       "      <td>-0.375577</td>\n",
       "      <td>-0.011666</td>\n",
       "      <td>-0.353730</td>\n",
       "      <td>0.131935</td>\n",
       "      <td>-0.431555</td>\n",
       "      <td>0.036446</td>\n",
       "      <td>-0.056576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polls ID                            Poll Responses Response  \\\n",
       "0     12522  The strengths of Plato's approach is his const...   \n",
       "1     12522  In the breakout we discussed if outside the ca...   \n",
       "2     12522  Back to cmmon confusion time: the section 'und...   \n",
       "3     12522  Most difficult weakness is that his position w...   \n",
       "4     12522  I'm still trying to understand the significanc...   \n",
       "\n",
       "  Assessment reports Hashtag  Assessment reports Score  time_stamp  \\\n",
       "0         #objectivemorality                         2           1   \n",
       "1         #objectivemorality                         3           1   \n",
       "2         #objectivemorality                         2           1   \n",
       "3         #objectivemorality                         2           1   \n",
       "4         #objectivemorality                         2           1   \n",
       "\n",
       "                                 tokenized_responses  \\\n",
       "0  ['The', 'strengths', 'of', \"Plato's\", 'approac...   \n",
       "1  ['In', 'the', 'breakout', 'we', 'discussed', '...   \n",
       "2  ['Back', 'to', 'cmmon', 'confusion', 'time', '...   \n",
       "3  ['Most', 'difficult', 'weakness', 'is', 'that'...   \n",
       "4  [\"I'm\", 'still', 'trying', 'to', 'understand',...   \n",
       "\n",
       "                                   stemmed_responses  \\\n",
       "0  ['the', 'strength', 'of', 'plato', 'approach',...   \n",
       "1  ['in', 'the', 'breakout', 'we', 'discuss', 'if...   \n",
       "2  ['back', 'to', 'cmmon', 'confus', 'time', 'the...   \n",
       "3  ['most', 'difficult', 'weak', 'is', 'that', 'h...   \n",
       "4  [\"i'm\", 'still', 'tri', 'to', 'understand', 't...   \n",
       "\n",
       "                                     clean_responses  \\\n",
       "0  ['strength', 'plato', 'approach', 'construct',...   \n",
       "1  ['breakout', 'discuss', 'outsid', 'cave', 'big...   \n",
       "2  ['back', 'cmmon', 'confus', 'time', 'section',...   \n",
       "3  ['difficult', 'weak', 'posit', 'understand', '...   \n",
       "4  [\"i'm\", 'still', 'tri', 'understand', 'signifi...   \n",
       "\n",
       "                                              string     LOs/ HCs  ...  \\\n",
       "0  strength plato approach construct whole framew...  objmorality  ...   \n",
       "1  breakout discuss outsid cave bigger cave thus ...  objmorality  ...   \n",
       "2  back cmmon confus time section understand inte...  objmorality  ...   \n",
       "3  difficult weak posit understand testabl like i...  objmorality  ...   \n",
       "4  i'm still tri understand signific cave analog ...  objmorality  ...   \n",
       "\n",
       "     vec_11    vec_12    vec_13    vec_14    vec_15    vec_16    vec_17  \\\n",
       "0  0.227378  0.228062 -0.065646 -0.018259  0.589726  0.165162  0.243406   \n",
       "1  0.067890 -0.172406 -0.026694 -0.313533  0.335412 -0.184949 -0.000822   \n",
       "2  0.172736  0.221275  0.215809 -0.060970  0.372512 -0.060175  0.251572   \n",
       "3  0.134589 -0.047511 -0.054336 -0.222005  0.227974 -0.272034  0.009888   \n",
       "4  0.025134  0.092645 -0.150749 -0.375577 -0.011666 -0.353730  0.131935   \n",
       "\n",
       "     vec_18    vec_19    vec_20  \n",
       "0  0.041697  0.047487 -0.080301  \n",
       "1  0.093128  0.022665  0.142223  \n",
       "2  0.366057  0.229787  0.318017  \n",
       "3 -0.173518  0.228723  0.078310  \n",
       "4 -0.431555  0.036446 -0.056576  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Assessment reports Score', axis=1)\n",
    "y = df['Assessment reports Score']\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "train_sample_indices = random.sample(list(X_train.index), k=1000)\n",
    "test_sample_indices = random.sample(list(X_test.index), k=100)\n",
    "\n",
    "small_X_train = X_train.loc[X_train.index.isin(train_sample_indices)]\n",
    "small_X_test = X_test.loc[X_test.index.isin(test_sample_indices)]\n",
    "small_y_train = y_train.loc[y_train.index.isin(train_sample_indices)]\n",
    "small_y_test = y_test.loc[y_test.index.isin(test_sample_indices)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Polls ID', 'Poll Responses Response', 'Assessment reports Hashtag',\n",
       "       'time_stamp', 'tokenized_responses', 'stemmed_responses',\n",
       "       'clean_responses', 'string', 'LOs/ HCs', 'College', 'Course',\n",
       "       'flesch_reading_ease', 'flesch_kincaid_grade', 'gunning_fog',\n",
       "       'smog_index', 'automated_readability_index', 'coleman_liau_index',\n",
       "       'dale_chall_readability_score', 'linsear_write_formula',\n",
       "       'lex_data_prep', 'hdd', 'Summer', 'Dugast', 'words_count',\n",
       "       'unique_words', 'ttr', 'rttr', 'cttr', 'mtld', 'herdan', 'maas', 'vec',\n",
       "       'vec_1', 'vec_2', 'vec_3', 'vec_4', 'vec_5', 'vec_6', 'vec_7', 'vec_8',\n",
       "       'vec_9', 'vec_10', 'vec_11', 'vec_12', 'vec_13', 'vec_14', 'vec_15',\n",
       "       'vec_16', 'vec_17', 'vec_18', 'vec_19', 'vec_20'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text input and create input tensors\n",
    "tokenized_texts = [tokenizer.tokenize(text) for text in df[\"Poll Responses Response\"]]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(text) for text in tokenized_texts]\n",
    "input_tensors = tf.keras.preprocessing.sequence.pad_sequences(input_ids, padding='post')\n",
    "\n",
    "# Create feature tensors\n",
    "feature_tensors = df[\"dale_chall_readability_score\"].values.reshape(-1, 1)\n",
    "\n",
    "# Concatenate input and feature tensors\n",
    "combined_tensors = tf.concat([input_tensors, feature_tensors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add batch dimension\n",
    "batch_tensors = [tensor.expand_dims(0) for tensor in combined_tensors]\n",
    "\n",
    "# Evaluate the sentiment of each input using BERT\n",
    "# sentiments = []\n",
    "# for batch_tensor in batch_tensors:\n",
    "#     outputs = model(batch_tensor)\n",
    "#     predictions = tf.argmax(outputs.logits, dim=1)\n",
    "#     sentiments.append(predictions.item())\n",
    "\n",
    "# # Add sentiment column to DataFrame\n",
    "# df[\"sentiment\"] = sentiments\n",
    "\n",
    "# # Save the DataFrame with sentiment predictions\n",
    "# df.to_csv(\"predictions.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 22:16:01.277988: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-20 22:16:01.278625: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.has_mps\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"Poll Responses Response\"], truncation=True)\n",
    " \n",
    "tokenized_train = tokenizer(list(small_X_train), truncation=True, padding=True, return_tensors = 'tf', max_length=128)\n",
    "tokenized_test = tokenizer(list(small_X_test), truncation=True, padding=True, return_tensors = 'tf', max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import DataCollatorWithPadding\n",
    "# import torch\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    " \n",
    "# def compute_metrics(eval_pred):\n",
    "#    load_accuracy = load_metric(\"accuracy\")\n",
    "#    load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "#    logits, labels = eval_pred\n",
    "#    predictions = np.argmax(logits, axis=-1)\n",
    "#    accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "#    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "#    return {\"accuracy\": accuracy, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tokenizer(vocabulary_size=30522, model=BertWordPiece, unk_token=[UNK], sep_token=[SEP], cls_token=[CLS], pad_token=[PAD], mask_token=[MASK], clean_text=True, handle_chinese_chars=True, strip_accents=None, lowercase=True, wordpieces_prefix=##)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "# First load the real tokenizer\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-uncased' , lower = True)\n",
    "# Save the loaded tokenizer locally\n",
    "tokenizer.save_pretrained('.')\n",
    "# Reload it with the huggingface tokenizers library\n",
    "fast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=True)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1000, 166), dtype=int32, numpy=\n",
      "array([[ 101, 2057, 2288, ...,    0,    0,    0],\n",
      "       [ 101, 2023, 2052, ...,    0,    0,    0],\n",
      "       [ 101, 1044, 2064, ...,    0,    0,    0],\n",
      "       ...,\n",
      "       [ 101, 2013, 2000, ...,    0,    0,    0],\n",
      "       [ 101, 2096, 1045, ...,    0,    0,    0],\n",
      "       [ 101, 2057, 2064, ...,    0,    0,    0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1000, 166), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "# Encode the training data\n",
    "encoded_train_data = tokenizer(list(small_X_train['Poll Responses Response']), padding=True, truncation=True, return_tensors='tf')\n",
    "encoded_test_data = tokenizer(list(small_X_test['Poll Responses Response']), padding=True, truncation=True, return_tensors='tf')\n",
    "\n",
    "# Print the encoded data\n",
    "print(encoded_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'distilbert']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:32<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from transformers import TFBertModel\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Get the number of samples in the data\n",
    "num_samples = len(encoded_train_data['input_ids'])\n",
    "\n",
    "# Create a list to store the embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Loop through the data in batches\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Get a batch of data\n",
    "    batch_input_ids = encoded_train_data['input_ids'][i:i+batch_size]\n",
    "    batch_attention_mask = encoded_train_data['attention_mask'][i:i+batch_size]\n",
    "    \n",
    "    # Get the embeddings for the batch\n",
    "    batch_outputs = bert_model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    \n",
    "    # Add the embeddings to the list\n",
    "    embeddings.extend(batch_outputs[0].numpy())\n",
    "\n",
    "# Convert the embeddings list to a numpy array\n",
    "embeddings_train = np.array(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:04<00:00,  7.98it/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings_test = []\n",
    "for i in tqdm(range(0, num_samples, batch_size)):\n",
    "    # Get a batch of data\n",
    "    batch_input_ids = encoded_test_data['input_ids'][i:i+batch_size]\n",
    "    batch_attention_mask = encoded_test_data['attention_mask'][i:i+batch_size]\n",
    "    \n",
    "    # Get the embeddings for the batch\n",
    "    batch_outputs = bert_model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "    \n",
    "    # Add the embeddings to the list\n",
    "    embeddings_test.extend(batch_outputs[0].numpy())\n",
    "\n",
    "# Convert the embeddings list to a numpy array\n",
    "embeddings_test = np.array(embeddings_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 166)]        0           []                               \n",
      "                                                                                                  \n",
      " input_34 (InputLayer)          [(None, 166)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_7 (TFBertModel)  TFBaseModelOutputWi  109482240   ['input_33[0][0]',               \n",
      "                                thPoolingAndCrossAt               'input_34[0][0]']               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 166,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_5 (Gl  (None, 768)         0           ['tf_bert_model_7[0][0]']        \n",
      " obalAveragePooling1D)                                                                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 1)            769         ['global_average_pooling1d_5[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,483,009\n",
      "Trainable params: 109,483,009\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "max_seq_length = 166\n",
    "# Create a Keras model that produces the BERT embeddings\n",
    "input_ids = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "attention_mask = tf.keras.Input(shape=(max_seq_length,), dtype=tf.int32)\n",
    "embedding_layer = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "pooling_layer = GlobalAveragePooling1D()(embedding_layer)\n",
    "output_layer = tf.keras.layers.Dense(1, activation='sigmoid')(pooling_layer)\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output_layer)\n",
    "\n",
    "model.summary()\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1000, 166]), TensorShape([1000, 166]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_data['input_ids'].shape,  encoded_train_data['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_y_train = np.asarray(small_y_train).astype('float32').reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 2/25 [=>............................] - ETA: 7:47 - loss: -3.4875 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      4\u001b[0m     x\u001b[39m=\u001b[39;49m[encoded_train_data[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], encoded_train_data[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m]],\n\u001b[1;32m      5\u001b[0m     y\u001b[39m=\u001b[39;49msmall_y_train,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     validation_split\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "batch_size = 32\n",
    "model.fit(\n",
    "    x=[encoded_train_data['input_ids'], encoded_train_data['attention_mask']],\n",
    "    y=small_y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=5,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras modules\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_one_hot = to_categorical(small_y_train)\n",
    "y_test_one_hot = to_categorical(small_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 166, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model_1\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 166, 768), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m input_ids \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[1;32m      3\u001b[0m attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m----> 4\u001b[0m bert_model([input_ids, attention_mask])\n\u001b[1;32m      5\u001b[0m \u001b[39m# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# bert_output, _ = bert_model([input_ids, attention_mask])\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# output = tf.keras.layers.Dense(5, activation='sigmoid')(bert_output)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m# model.summary()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filetjm3dlko.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model_1\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 166, 768), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define the model architecture\n",
    "input_ids = tf.keras.Input(shape=(embeddings_train.shape[1], embeddings_train.shape[2]), dtype=tf.int32)\n",
    "attention_mask = tf.keras.Input(shape=(embeddings_train.shape[1], embeddings_train.shape[2]), dtype=tf.int32)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_model([input_ids, attention_mask])\n",
    "\n",
    "# bert_output, _ = bert_model([input_ids, attention_mask])\n",
    "# output = tf.keras.layers.Dense(5, activation='sigmoid')(bert_output)\n",
    "# model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# # compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model_1\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 166, 768), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], embeddings_train\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[1;32m      7\u001b[0m bert_model \u001b[39m=\u001b[39m TFBertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m bert_output \u001b[39m=\u001b[39m bert_model([input_ids, attention_mask])[\u001b[39m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m5\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)(bert_output)\n\u001b[1;32m     10\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[input_ids, attention_mask], outputs\u001b[39m=\u001b[39moutput)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filetjm3dlko.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_bert_model_1\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model_1\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 166, 768), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "\n",
    "# define the model architecture\n",
    "input_ids = tf.keras.Input(shape=(embeddings_train.shape[1], embeddings_train.shape[2]), dtype=tf.int32)\n",
    "attention_mask = tf.keras.Input(shape=(embeddings_train.shape[1], embeddings_train.shape[2]), dtype=tf.int32)\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "bert_output = bert_model([input_ids, attention_mask])[1]\n",
    "output = tf.keras.layers.Dense(5, activation='sigmoid')(bert_output)\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf_bert_model\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 128, 128), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 128, 128), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# input_ids = tf.keras.Input(embeddings_train.shape[1], dtype=tf.int32)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m attention_mask \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39m128\u001b[39m,\u001b[39m128\u001b[39m), dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m----> 6\u001b[0m bert_output \u001b[39m=\u001b[39m bert_model([input_ids, attention_mask])[\u001b[39m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m output \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m5\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)(bert_output)\n\u001b[1;32m      8\u001b[0m model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mModel(inputs\u001b[39m=\u001b[39m[input_ids, attention_mask], outputs\u001b[39m=\u001b[39moutput)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filetjm3dlko.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     28\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     29\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 30\u001b[0m outputs \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mbert, (), \u001b[39mdict\u001b[39m(input_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(input_ids), attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(attention_mask), token_type_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(token_type_ids), position_ids\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(position_ids), head_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(head_mask), inputs_embeds\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(inputs_embeds), encoder_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_hidden_states), encoder_attention_mask\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(encoder_attention_mask), past_key_values\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(past_key_values), use_cache\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(use_cache), output_attentions\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_attentions), output_hidden_states\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(output_hidden_states), return_dict\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(return_dict), training\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mld(training)), fscope)\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py:36\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(func), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m),), \u001b[39mdict\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mag__\u001b[39m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     37\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py:75\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m     73\u001b[0m input_shape \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     74\u001b[0m ag__\u001b[39m.\u001b[39mif_stmt(ag__\u001b[39m.\u001b[39mand_(\u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(input_ids) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39mlambda\u001b[39;00m : ag__\u001b[39m.\u001b[39mld(inputs_embeds) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m), if_body_3, else_body_3, get_state_3, set_state_3, (\u001b[39m'\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m'\u001b[39m,), \u001b[39m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m (batch_size, seq_length) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(input_shape)\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_4\u001b[39m():\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m (past_key_values, past_key_values_length)\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer \"tf_bert_model\" (type TFBertModel).\n\nin user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n        return func(self, **unpacked_inputs)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 1115, in call  *\n        outputs = self.bert(\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file2tfwx03d.py\", line 36, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem41a2rss.py\", line 75, in tf__call\n        (batch_size, seq_length) = ag__.ld(input_shape)\n\n    ValueError: Exception encountered when calling layer \"bert\" (type TFBertMainLayer).\n    \n    in user code:\n    \n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py\", line 1088, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py\", line 775, in call  *\n            batch_size, seq_length = input_shape\n    \n        ValueError: too many values to unpack (expected 2)\n    \n    \n    Call arguments received by layer \"bert\" (type TFBertMainLayer):\n      â€¢ self=tf.Tensor(shape=(None, 166, 768), dtype=int32)\n      â€¢ input_ids=None\n      â€¢ attention_mask=tf.Tensor(shape=(None, 128, 128), dtype=int32)\n      â€¢ token_type_ids=None\n      â€¢ position_ids=None\n      â€¢ head_mask=None\n      â€¢ inputs_embeds=None\n      â€¢ encoder_hidden_states=None\n      â€¢ encoder_attention_mask=None\n      â€¢ past_key_values=None\n      â€¢ use_cache=True\n      â€¢ output_attentions=False\n      â€¢ output_hidden_states=False\n      â€¢ return_dict=True\n      â€¢ training=False\n\n\nCall arguments received by layer \"tf_bert_model\" (type TFBertModel):\n  â€¢ self=['tf.Tensor(shape=(None, 166, 768), dtype=int32)', 'tf.Tensor(shape=(None, 128, 128), dtype=int32)']\n  â€¢ input_ids=None\n  â€¢ attention_mask=None\n  â€¢ token_type_ids=None\n  â€¢ position_ids=None\n  â€¢ head_mask=None\n  â€¢ inputs_embeds=None\n  â€¢ encoder_hidden_states=None\n  â€¢ encoder_attention_mask=None\n  â€¢ past_key_values=None\n  â€¢ use_cache=None\n  â€¢ output_attentions=None\n  â€¢ output_hidden_states=None\n  â€¢ return_dict=None\n  â€¢ training=False"
     ]
    }
   ],
   "source": [
    "# define the model architecture\n",
    "input_ids = tf.keras.Input(shape=(embeddings_train.shape[1], embeddings_train.shape[2]), dtype=tf.int32)\n",
    "\n",
    "# input_ids = tf.keras.Input(embeddings_train.shape[1], dtype=tf.int32)\n",
    "attention_mask = tf.keras.Input(shape=(128, ), dtype=tf.int32)\n",
    "bert_output = bert_model([input_ids, attention_mask])[1]\n",
    "output = tf.keras.layers.Dense(5, activation='sigmoid')(bert_output)\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "# compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=2e-5)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 22:16:49.676590: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 166, 768) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(embeddings_train, y_train_one_hot, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(embeddings_test, y_test_one_hot))\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_file80gy_k8q.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 200, in assert_input_compatibility\n        raise ValueError(f'Layer \"{layer_name}\" expects {len(input_spec)} input(s),'\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 166, 768) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(embeddings_train, y_train_one_hot, epochs=10, validation_data=(embeddings_test, y_test_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-20 22:00:26.269041: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 166) for input KerasTensor(type_spec=TensorSpec(shape=(None, 166), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (None, 166, 768).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 248, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 166, but received input with shape (None, 166, 768)\n    \n    Call arguments received by layer \"model\" (type Functional):\n      â€¢ inputs=tf.Tensor(shape=(None, 166, 768), dtype=float32)\n      â€¢ training=True\n      â€¢ mask=None\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mfit(embeddings_train, y_train_one_hot, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m \u001b[39m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     16\u001b[0m test_loss, test_accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(embeddings_test, y_test_one_hot, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/__autograph_generated_filem11nmeeu.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/swimmingcircle/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/input_spec.py\", line 248, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer \"model\" (type Functional).\n    \n    Input 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 166, but received input with shape (None, 166, 768)\n    \n    Call arguments received by layer \"model\" (type Functional):\n      â€¢ inputs=tf.Tensor(shape=(None, 166, 768), dtype=float32)\n      â€¢ training=True\n      â€¢ mask=None\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network architecture\n",
    "num_classes = 5\n",
    "inputs = Input(shape=(embeddings_train.shape[1],))\n",
    "x = Dense(64, activation='relu')(inputs)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(embeddings_train, y_train_one_hot, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(embeddings_test, y_test_one_hot, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda3c93c9c3a42bf985b8918f2ba6e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tf_model.h5:   0%|          | 0.00/363M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFBertModel: ['vocab_transform', 'vocab_projector', 'distilbert', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['bert']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m bert_model \u001b[39m=\u001b[39m TFBertModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mdistilbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Get the embeddings for the encoded data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m outputs \u001b[39m=\u001b[39m bert_model(encoded_train_data[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m], attention_mask\u001b[39m=\u001b[39;49mencoded_train_data[\u001b[39m'\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:490\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(inputs, \u001b[39m*\u001b[39mcopied_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcopied_kwargs)\n\u001b[1;32m    488\u001b[0m   layout_map_lib\u001b[39m.\u001b[39m_map_subclass_model_variable(\u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_layout_map)\n\u001b[0;32m--> 490\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:1115\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[39m@unpack_inputs\u001b[39m\n\u001b[1;32m   1072\u001b[0m \u001b[39m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[39m.\u001b[39mformat(\u001b[39m\"\u001b[39m\u001b[39mbatch_size, sequence_length\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1073\u001b[0m \u001b[39m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     training: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[39m.\u001b[39mTensor]]:\n\u001b[1;32m   1095\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[39m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[39m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[1;32m   1116\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1117\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1118\u001b[0m         token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1119\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1120\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1121\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1122\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1123\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1124\u001b[0m         past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1125\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1126\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1127\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1128\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1129\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m   1130\u001b[0m     )\n\u001b[1;32m   1131\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[39m=\u001b[39m input_processing(func, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49munpacked_inputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:871\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     head_mask \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 871\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    872\u001b[0m     hidden_states\u001b[39m=\u001b[39;49membedding_output,\n\u001b[1;32m    873\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    874\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    875\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    876\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    877\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    878\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    879\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    880\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    881\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    882\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    883\u001b[0m )\n\u001b[1;32m    885\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(hidden_states\u001b[39m=\u001b[39msequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:562\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    558\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_states,)\n\u001b[1;32m    560\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[i] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    563\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    565\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    566\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    567\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    568\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    570\u001b[0m     training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    571\u001b[0m )\n\u001b[1;32m    572\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:472\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    460\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    461\u001b[0m     hidden_states: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    473\u001b[0m         input_tensor\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    474\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    475\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    476\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    477\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    478\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    479\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    480\u001b[0m         training\u001b[39m=\u001b[39;49mtraining,\n\u001b[1;32m    481\u001b[0m     )\n\u001b[1;32m    482\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:398\u001b[0m, in \u001b[0;36mTFBertAttention.call\u001b[0;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     input_tensor: tf\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     training: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[tf\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    388\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attention(\n\u001b[1;32m    389\u001b[0m         hidden_states\u001b[39m=\u001b[39minput_tensor,\n\u001b[1;32m    390\u001b[0m         attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m         training\u001b[39m=\u001b[39mtraining,\n\u001b[1;32m    397\u001b[0m     )\n\u001b[0;32m--> 398\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense_output(\n\u001b[1;32m    399\u001b[0m         hidden_states\u001b[39m=\u001b[39;49mself_outputs[\u001b[39m0\u001b[39;49m], input_tensor\u001b[39m=\u001b[39;49minput_tensor, training\u001b[39m=\u001b[39;49mtraining\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    401\u001b[0m     \u001b[39m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n\u001b[1;32m    402\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:362\u001b[0m, in \u001b[0;36mTFBertSelfOutput.call\u001b[0;34m(self, hidden_states, input_tensor, training)\u001b[0m\n\u001b[1;32m    360\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(inputs\u001b[39m=\u001b[39mhidden_states)\n\u001b[1;32m    361\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(inputs\u001b[39m=\u001b[39mhidden_states, training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m--> 362\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLayerNorm(inputs\u001b[39m=\u001b[39;49mhidden_states \u001b[39m+\u001b[39;49m input_tensor)\n\u001b[1;32m    364\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/engine/base_layer.py:1014\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1010\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1012\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1013\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1014\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1016\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1017\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     93\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     94\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     95\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/keras/layers/normalization/layer_normalization.py:275\u001b[0m, in \u001b[0;36mLayerNormalization.call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    272\u001b[0m   scale, offset \u001b[39m=\u001b[39m _broadcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma), _broadcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta)\n\u001b[1;32m    274\u001b[0m   \u001b[39m# Compute layer normalization using the batch_normalization function.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mbatch_normalization(\n\u001b[1;32m    276\u001b[0m       inputs,\n\u001b[1;32m    277\u001b[0m       mean,\n\u001b[1;32m    278\u001b[0m       variance,\n\u001b[1;32m    279\u001b[0m       offset\u001b[39m=\u001b[39;49moffset,\n\u001b[1;32m    280\u001b[0m       scale\u001b[39m=\u001b[39;49mscale,\n\u001b[1;32m    281\u001b[0m       variance_epsilon\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon)\n\u001b[1;32m    282\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(outputs, input_dtype)\n\u001b[1;32m    283\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    284\u001b[0m   \u001b[39m# Collapse dims before self.axis, and dims in self.axis\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/nn_impl.py:1587\u001b[0m, in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1585\u001b[0m inv \u001b[39m=\u001b[39m math_ops\u001b[39m.\u001b[39mrsqrt(variance \u001b[39m+\u001b[39m variance_epsilon)\n\u001b[1;32m   1586\u001b[0m \u001b[39mif\u001b[39;00m scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1587\u001b[0m   inv \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m scale\n\u001b[1;32m   1588\u001b[0m \u001b[39m# Note: tensorflow/contrib/quantize/python/fold_batch_norms.py depends on\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m \u001b[39m# the precise order of ops that are generated by the expression below.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39m math_ops\u001b[39m.\u001b[39mcast(inv, x\u001b[39m.\u001b[39mdtype) \u001b[39m+\u001b[39m math_ops\u001b[39m.\u001b[39mcast(\n\u001b[1;32m   1591\u001b[0m     offset \u001b[39m-\u001b[39m mean \u001b[39m*\u001b[39m inv \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39mmean \u001b[39m*\u001b[39m inv, x\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1406\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1401\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1402\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[1;32m   1403\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1404\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m   1405\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m-> 1406\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1407\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1408\u001b[0m   \u001b[39m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m   \u001b[39m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1412\u001b[0m   \u001b[39m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[1;32m   1413\u001b[0m   \u001b[39m# informative.\u001b[39;00m\n\u001b[1;32m   1414\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mtype\u001b[39m(y), \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name):\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1766\u001b[0m, in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1764\u001b[0m   \u001b[39mreturn\u001b[39;00m sparse_tensor\u001b[39m.\u001b[39mSparseTensor(y\u001b[39m.\u001b[39mindices, new_vals, y\u001b[39m.\u001b[39mdense_shape)\n\u001b[1;32m   1765\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1766\u001b[0m   \u001b[39mreturn\u001b[39;00m multiply(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1082\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1082\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1083\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1084\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:529\u001b[0m, in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmath.multiply\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmultiply\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    481\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39mregister_binary_elementwise_api\n\u001b[1;32m    482\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[1;32m    483\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmultiply\u001b[39m(x, y, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    484\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns an element-wise x * y.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \n\u001b[1;32m    486\u001b[0m \u001b[39m  For example:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[39m   * InvalidArgumentError: When `x` and `y` have incompatible shapes or types.\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 529\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmul(x, y, name)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:6576\u001b[0m, in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6574\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   6575\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6576\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   6577\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, x, y)\n\u001b[1;32m   6578\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   6579\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# from transformers import TFBertModel\n",
    "\n",
    "# # Load the pre-trained BERT model\n",
    "# bert_model = TFBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# # Get the embeddings for the encoded data\n",
    "# outputs = bert_model(encoded_train_data['input_ids'], attention_mask=encoded_train_data['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
