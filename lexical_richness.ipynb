{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexicalrichness import LexicalRichness\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('the_rest_data.zip')\n",
    "df = df[df['Poll Responses Response'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Polls ID', 'Poll Responses Response', 'Assessment reports Hashtag',\n",
       "       'Assessment reports Score', 'time_stamp', 'tokenized_responses',\n",
       "       'stemmed_responses', 'clean_responses', 'string', 'LOs/ HCs', 'College',\n",
       "       'Course', 'flesch_reading_ease', 'flesch_kincaid_grade', 'gunning_fog',\n",
       "       'smog_index', 'automated_readability_index', 'coleman_liau_index',\n",
       "       'dale_chall_readability_score', 'linsear_write_formula'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean This student was present, but did not complete the poll.\n",
    "df = df[df['Poll Responses Response'] != 'This student was present, but did not complete the poll.']\n",
    "\n",
    "#clean up extremely short sentences\n",
    "df = df[df['string'].str.len() >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df['string'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polls ID</th>\n",
       "      <th>Poll Responses Response</th>\n",
       "      <th>Assessment reports Hashtag</th>\n",
       "      <th>Assessment reports Score</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>tokenized_responses</th>\n",
       "      <th>stemmed_responses</th>\n",
       "      <th>clean_responses</th>\n",
       "      <th>string</th>\n",
       "      <th>LOs/ HCs</th>\n",
       "      <th>College</th>\n",
       "      <th>Course</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>99333</td>\n",
       "      <td>beta, gamma and rate (nu)</td>\n",
       "      <td>#exactinference</td>\n",
       "      <td>1</td>\n",
       "      <td>3141</td>\n",
       "      <td>['beta', 'gamma', 'and', 'rate', 'nu']</td>\n",
       "      <td>['beta', 'gamma', 'and', 'rate', 'nu']</td>\n",
       "      <td>['beta', 'gamma', 'rate', 'nu']</td>\n",
       "      <td>beta gamma rate nu</td>\n",
       "      <td>decisioninference</td>\n",
       "      <td>CS</td>\n",
       "      <td>CS112</td>\n",
       "      <td>75.88</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>15.68</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3977</th>\n",
       "      <td>16158</td>\n",
       "      <td>c because they both have a real</td>\n",
       "      <td>#evaluatephylogenies</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>['c', 'because', 'they', 'both', 'have', 'a', ...</td>\n",
       "      <td>['c', 'becaus', 'they', 'both', 'have', 'a', '...</td>\n",
       "      <td>['c', 'becaus', 'real']</td>\n",
       "      <td>c becaus real</td>\n",
       "      <td>evaluatephylogenies</td>\n",
       "      <td>NS</td>\n",
       "      <td>NS112</td>\n",
       "      <td>59.97</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>-4.29</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4018</th>\n",
       "      <td>48922</td>\n",
       "      <td>C is wrong because</td>\n",
       "      <td>#evaluatephylogenies</td>\n",
       "      <td>1</td>\n",
       "      <td>545</td>\n",
       "      <td>['C', 'is', 'wrong', 'because']</td>\n",
       "      <td>['c', 'is', 'wrong', 'becaus']</td>\n",
       "      <td>['c', 'wrong', 'becaus']</td>\n",
       "      <td>c wrong becaus</td>\n",
       "      <td>evaluatephylogenies</td>\n",
       "      <td>NS</td>\n",
       "      <td>NS112</td>\n",
       "      <td>93.81</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4023</th>\n",
       "      <td>48922</td>\n",
       "      <td>A - pine stemmed from Fern</td>\n",
       "      <td>#evaluatephylogenies</td>\n",
       "      <td>2</td>\n",
       "      <td>545</td>\n",
       "      <td>['A', 'pine', 'stemmed', 'from', 'Fern']</td>\n",
       "      <td>['a', 'pine', 'stem', 'from', 'fern']</td>\n",
       "      <td>['pine', 'stem', 'fern']</td>\n",
       "      <td>pine stem fern</td>\n",
       "      <td>evaluatephylogenies</td>\n",
       "      <td>NS</td>\n",
       "      <td>NS112</td>\n",
       "      <td>119.19</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>9.05</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>265602</td>\n",
       "      <td>Red Kangaro because they</td>\n",
       "      <td>#evaluatephylogenies</td>\n",
       "      <td>3</td>\n",
       "      <td>12308</td>\n",
       "      <td>['Red', 'Kangaro', 'because', 'they']</td>\n",
       "      <td>['red', 'kangaro', 'becaus', 'they']</td>\n",
       "      <td>['red', 'kangaro', 'becaus']</td>\n",
       "      <td>red kangaro becaus</td>\n",
       "      <td>evaluatephylogenies</td>\n",
       "      <td>NS</td>\n",
       "      <td>NS112</td>\n",
       "      <td>59.97</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179123</th>\n",
       "      <td>293739</td>\n",
       "      <td>(,) --&gt; (,)\\n(,) --&gt; (,)\\n\\nmap: T(,)\\n\\nTo ca...</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>2</td>\n",
       "      <td>13709</td>\n",
       "      <td>['map', 'T', 'To', 'calculate', 'T', 'T']</td>\n",
       "      <td>['map', 't', 'to', 'calcul', 't', 't']</td>\n",
       "      <td>['map', 'calcul']</td>\n",
       "      <td>map calcul</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>CS</td>\n",
       "      <td>CS111B</td>\n",
       "      <td>77.91</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-4.51</td>\n",
       "      <td>11.63</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179209</th>\n",
       "      <td>297683</td>\n",
       "      <td>I sincerely do not know.</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>1</td>\n",
       "      <td>14012</td>\n",
       "      <td>['I', 'sincerely', 'do', 'not', 'know']</td>\n",
       "      <td>['i', 'sincer', 'do', 'not', 'know']</td>\n",
       "      <td>['sincer', 'know']</td>\n",
       "      <td>sincer know</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>CS</td>\n",
       "      <td>CS111B</td>\n",
       "      <td>77.91</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-1.61</td>\n",
       "      <td>11.63</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179241</th>\n",
       "      <td>303417</td>\n",
       "      <td>RREF of {(, , ), (, , ), (, -, )} {, , }\\nR = ...</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>3</td>\n",
       "      <td>14344</td>\n",
       "      <td>['RREF', 'of', 'R', 'R', 'R']</td>\n",
       "      <td>['rref', 'of', 'r', 'r', 'r']</td>\n",
       "      <td>['rref', 'r', 'r', 'r']</td>\n",
       "      <td>rref r r r</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>CS</td>\n",
       "      <td>CS111B</td>\n",
       "      <td>118.18</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.3</td>\n",
       "      <td>-13.06</td>\n",
       "      <td>11.73</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179375</th>\n",
       "      <td>305210</td>\n",
       "      <td>s = [,,],[,,],[,,]\\n[,,] = a[,,]+b[,,]+c[-,,]\\...</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>3</td>\n",
       "      <td>14435</td>\n",
       "      <td>['s', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'g', ...</td>\n",
       "      <td>['s', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'g', ...</td>\n",
       "      <td>['b', 'c', 'e', 'f', 'g', 'g', 'v']</td>\n",
       "      <td>b c e f g g v</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>CS</td>\n",
       "      <td>CS111B</td>\n",
       "      <td>115.13</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>-14.15</td>\n",
       "      <td>17.52</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179744</th>\n",
       "      <td>318627</td>\n",
       "      <td>The isotherm most likely has a kink at</td>\n",
       "      <td>#PhaseTransitions</td>\n",
       "      <td>2</td>\n",
       "      <td>15126</td>\n",
       "      <td>['The', 'isotherm', 'most', 'likely', 'has', '...</td>\n",
       "      <td>['the', 'isotherm', 'most', 'like', 'has', 'a'...</td>\n",
       "      <td>['isotherm', 'like', 'kink']</td>\n",
       "      <td>isotherm like kink</td>\n",
       "      <td>phasetransitions</td>\n",
       "      <td>NS</td>\n",
       "      <td>NS162</td>\n",
       "      <td>119.19</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Polls ID                            Poll Responses Response  \\\n",
       "1144       99333                          beta, gamma and rate (nu)   \n",
       "3977       16158                    c because they both have a real   \n",
       "4018       48922                                 C is wrong because   \n",
       "4023       48922                         A - pine stemmed from Fern   \n",
       "4141      265602                           Red Kangaro because they   \n",
       "...          ...                                                ...   \n",
       "179123    293739  (,) --> (,)\\n(,) --> (,)\\n\\nmap: T(,)\\n\\nTo ca...   \n",
       "179209    297683                           I sincerely do not know.   \n",
       "179241    303417  RREF of {(, , ), (, , ), (, -, )} {, , }\\nR = ...   \n",
       "179375    305210  s = [,,],[,,],[,,]\\n[,,] = a[,,]+b[,,]+c[-,,]\\...   \n",
       "179744    318627             The isotherm most likely has a kink at   \n",
       "\n",
       "       Assessment reports Hashtag  Assessment reports Score  time_stamp  \\\n",
       "1144              #exactinference                         1        3141   \n",
       "3977         #evaluatephylogenies                         2          18   \n",
       "4018         #evaluatephylogenies                         1         545   \n",
       "4023         #evaluatephylogenies                         2         545   \n",
       "4141         #evaluatephylogenies                         3       12308   \n",
       "...                           ...                       ...         ...   \n",
       "179123           #Transformations                         2       13709   \n",
       "179209           #Transformations                         1       14012   \n",
       "179241           #Transformations                         3       14344   \n",
       "179375           #Transformations                         3       14435   \n",
       "179744          #PhaseTransitions                         2       15126   \n",
       "\n",
       "                                      tokenized_responses  \\\n",
       "1144               ['beta', 'gamma', 'and', 'rate', 'nu']   \n",
       "3977    ['c', 'because', 'they', 'both', 'have', 'a', ...   \n",
       "4018                      ['C', 'is', 'wrong', 'because']   \n",
       "4023             ['A', 'pine', 'stemmed', 'from', 'Fern']   \n",
       "4141                ['Red', 'Kangaro', 'because', 'they']   \n",
       "...                                                   ...   \n",
       "179123          ['map', 'T', 'To', 'calculate', 'T', 'T']   \n",
       "179209            ['I', 'sincerely', 'do', 'not', 'know']   \n",
       "179241                      ['RREF', 'of', 'R', 'R', 'R']   \n",
       "179375  ['s', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'g', ...   \n",
       "179744  ['The', 'isotherm', 'most', 'likely', 'has', '...   \n",
       "\n",
       "                                        stemmed_responses  \\\n",
       "1144               ['beta', 'gamma', 'and', 'rate', 'nu']   \n",
       "3977    ['c', 'becaus', 'they', 'both', 'have', 'a', '...   \n",
       "4018                       ['c', 'is', 'wrong', 'becaus']   \n",
       "4023                ['a', 'pine', 'stem', 'from', 'fern']   \n",
       "4141                 ['red', 'kangaro', 'becaus', 'they']   \n",
       "...                                                   ...   \n",
       "179123             ['map', 't', 'to', 'calcul', 't', 't']   \n",
       "179209               ['i', 'sincer', 'do', 'not', 'know']   \n",
       "179241                      ['rref', 'of', 'r', 'r', 'r']   \n",
       "179375  ['s', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'g', ...   \n",
       "179744  ['the', 'isotherm', 'most', 'like', 'has', 'a'...   \n",
       "\n",
       "                            clean_responses              string  \\\n",
       "1144        ['beta', 'gamma', 'rate', 'nu']  beta gamma rate nu   \n",
       "3977                ['c', 'becaus', 'real']       c becaus real   \n",
       "4018               ['c', 'wrong', 'becaus']      c wrong becaus   \n",
       "4023               ['pine', 'stem', 'fern']      pine stem fern   \n",
       "4141           ['red', 'kangaro', 'becaus']  red kangaro becaus   \n",
       "...                                     ...                 ...   \n",
       "179123                    ['map', 'calcul']          map calcul   \n",
       "179209                   ['sincer', 'know']         sincer know   \n",
       "179241              ['rref', 'r', 'r', 'r']          rref r r r   \n",
       "179375  ['b', 'c', 'e', 'f', 'g', 'g', 'v']       b c e f g g v   \n",
       "179744         ['isotherm', 'like', 'kink']  isotherm like kink   \n",
       "\n",
       "                         LOs/ HCs College  Course  flesch_reading_ease  \\\n",
       "1144            decisioninference      CS   CS112                75.88   \n",
       "3977          evaluatephylogenies      NS   NS112                59.97   \n",
       "4018          evaluatephylogenies      NS   NS112                93.81   \n",
       "4023          evaluatephylogenies      NS   NS112               119.19   \n",
       "4141          evaluatephylogenies      NS   NS112                59.97   \n",
       "...                           ...     ...     ...                  ...   \n",
       "179123  transformationapplication      CS  CS111B                77.91   \n",
       "179209  transformationapplication      CS  CS111B                77.91   \n",
       "179241  transformationapplication      CS  CS111B               118.18   \n",
       "179375  transformationapplication      CS  CS111B               115.13   \n",
       "179744           phasetransitions      NS   NS162               119.19   \n",
       "\n",
       "        flesch_kincaid_grade  gunning_fog  smog_index  \\\n",
       "1144                     3.7          1.6         0.0   \n",
       "3977                     5.6          1.2         0.0   \n",
       "4018                     0.9          1.2         0.0   \n",
       "4023                    -2.7          1.2         0.0   \n",
       "4141                     5.6          1.2         0.0   \n",
       "...                      ...          ...         ...   \n",
       "179123                   2.9          0.8         0.0   \n",
       "179209                   2.9          0.8         0.0   \n",
       "179241                  -2.3          1.6         0.0   \n",
       "179375                  -1.2          2.8         0.0   \n",
       "179744                  -2.7          1.2         0.0   \n",
       "\n",
       "        automated_readability_index  coleman_liau_index  \\\n",
       "1144                           -1.9               -1.46   \n",
       "3977                           -2.7               -4.29   \n",
       "4018                           -1.2               -2.38   \n",
       "4023                           -1.2               -2.38   \n",
       "4141                            5.2                5.35   \n",
       "...                             ...                 ...   \n",
       "179123                          0.8               -4.51   \n",
       "179209                          3.1               -1.61   \n",
       "179241                        -11.3              -13.06   \n",
       "179375                        -13.3              -14.15   \n",
       "179744                          5.2                5.35   \n",
       "\n",
       "        dale_chall_readability_score  linsear_write_formula  \n",
       "1144                           15.68                    1.0  \n",
       "3977                           14.31                    0.5  \n",
       "4018                           14.31                    0.5  \n",
       "4023                            9.05                    0.5  \n",
       "4141                           14.31                    0.5  \n",
       "...                              ...                    ...  \n",
       "179123                         11.63                    0.0  \n",
       "179209                         11.63                    0.0  \n",
       "179241                         11.73                    1.0  \n",
       "179375                         17.52                    2.5  \n",
       "179744                         14.31                    0.5  \n",
       "\n",
       "[307 rows x 20 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['string'].str.len() <= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not yet put in lex.Dugast, lex.msttr(segment_window=0), lex.mattr(window_size=0),lex.hdd(draws=0),\n",
    "def generate_words(text):\n",
    "    lex = LexicalRichness(text)\n",
    "    return lex.words\n",
    "df['words_count'] = df['string'].apply(generate_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['words_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lex_data_prep']= df['stemmed_responses'].str.split(',').replace(\"'\", '').str.join(' ')\n",
    "\n",
    "df['lex_data_prep'] = df['stemmed_responses'].apply(\" \".join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the  strength  of  plato  approach  is  his  c...\n",
       "1    in  the  breakout  we  discuss  if  outsid  th...\n",
       "2    back  to  cmmon  confus  time  the  section  u...\n",
       "3    most  difficult  weak  is  that  his  posit  w...\n",
       "4    \"im\"  still  tri  to  understand  the  signifi...\n",
       "Name: lex_data_prep, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['lex_data_prep'] = df['stemmed_responses'].str.strip('[').str.strip(']').str.split(',').str.join(' ').str.replace(\"'\", '')\n",
    "df['lex_data_prep'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polls ID</th>\n",
       "      <th>Poll Responses Response</th>\n",
       "      <th>Assessment reports Hashtag</th>\n",
       "      <th>Assessment reports Score</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>tokenized_responses</th>\n",
       "      <th>stemmed_responses</th>\n",
       "      <th>clean_responses</th>\n",
       "      <th>string</th>\n",
       "      <th>LOs/ HCs</th>\n",
       "      <th>...</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>words_count</th>\n",
       "      <th>lex_data_prep</th>\n",
       "      <th>Dugast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>108779</td>\n",
       "      <td>Threat for two reasons ) explanatory power.</td>\n",
       "      <td>#objmorality</td>\n",
       "      <td>4</td>\n",
       "      <td>4028</td>\n",
       "      <td>['Threat', 'for', 'two', 'reasons', 'explanato...</td>\n",
       "      <td>['threat', 'for', 'two', 'reason', 'explanator...</td>\n",
       "      <td>['threat', 'two', 'reason', 'explanatori', 'po...</td>\n",
       "      <td>threat two reason explanatori power</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>14.24</td>\n",
       "      <td>10.20</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5</td>\n",
       "      <td>threat  for  two  reason  explanatori  power</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>92219</td>\n",
       "      <td>Multiply by exp and then I'd integrate over th...</td>\n",
       "      <td>#exactinference</td>\n",
       "      <td>2</td>\n",
       "      <td>2207</td>\n",
       "      <td>['Multiply', 'by', 'exp', 'and', 'then', \"I'd\"...</td>\n",
       "      <td>['multipli', 'by', 'exp', 'and', 'then', \"i'd\"...</td>\n",
       "      <td>['multipli', 'exp', \"i'd\", 'integr', 'theta', ...</td>\n",
       "      <td>multipli exp i'd integr theta margin</td>\n",
       "      <td>decisioninference</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>8.17</td>\n",
       "      <td>17.09</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>multipli  by  exp  and  then  \"id\"  integr  ov...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>92219</td>\n",
       "      <td>In general, I would treat θ as a nuisance para...</td>\n",
       "      <td>#exactinference</td>\n",
       "      <td>2</td>\n",
       "      <td>2207</td>\n",
       "      <td>['In', 'general', 'I', 'would', 'treat', 'θ', ...</td>\n",
       "      <td>['in', 'general', 'i', 'would', 'treat', 'θ', ...</td>\n",
       "      <td>['general', 'treat', 'θ', 'nuisanc', 'paramet'...</td>\n",
       "      <td>general treat θ nuisanc paramet sum integr</td>\n",
       "      <td>decisioninference</td>\n",
       "      <td>...</td>\n",
       "      <td>9.6</td>\n",
       "      <td>8.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>9.87</td>\n",
       "      <td>13.01</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>in  general  i  would  treat  θ  as  a  nuisan...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1144</th>\n",
       "      <td>99333</td>\n",
       "      <td>beta, gamma and rate (nu)</td>\n",
       "      <td>#exactinference</td>\n",
       "      <td>1</td>\n",
       "      <td>3141</td>\n",
       "      <td>['beta', 'gamma', 'and', 'rate', 'nu']</td>\n",
       "      <td>['beta', 'gamma', 'and', 'rate', 'nu']</td>\n",
       "      <td>['beta', 'gamma', 'rate', 'nu']</td>\n",
       "      <td>beta gamma rate nu</td>\n",
       "      <td>decisioninference</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.9</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>15.68</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>beta  gamma  and  rate  nu</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149</th>\n",
       "      <td>99335</td>\n",
       "      <td>I'm still confused about the last activity.</td>\n",
       "      <td>#exactinference</td>\n",
       "      <td>1</td>\n",
       "      <td>3142</td>\n",
       "      <td>[\"I'm\", 'still', 'confused', 'about', 'the', '...</td>\n",
       "      <td>[\"i'm\", 'still', 'confus', 'about', 'the', 'la...</td>\n",
       "      <td>[\"i'm\", 'still', 'confus', 'last', 'activ']</td>\n",
       "      <td>i'm still confus last activ</td>\n",
       "      <td>decisioninference</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.80</td>\n",
       "      <td>10.20</td>\n",
       "      <td>1.5</td>\n",
       "      <td>6</td>\n",
       "      <td>\"im\"  still  confus  about  the  last  activ</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179327</th>\n",
       "      <td>305203</td>\n",
       "      <td>The second one hold when we draw ou</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>1</td>\n",
       "      <td>14433</td>\n",
       "      <td>['The', 'second', 'one', 'hold', 'when', 'we',...</td>\n",
       "      <td>['the', 'second', 'one', 'hold', 'when', 'we',...</td>\n",
       "      <td>['second', 'one', 'hold', 'draw', 'ou']</td>\n",
       "      <td>second one hold draw ou</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>7.04</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5</td>\n",
       "      <td>the  second  one  hold  when  we  draw  ou</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179390</th>\n",
       "      <td>310232</td>\n",
       "      <td>It means that the rows must form a set of orth...</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>2</td>\n",
       "      <td>14630</td>\n",
       "      <td>['It', 'means', 'that', 'the', 'rows', 'must',...</td>\n",
       "      <td>['it', 'mean', 'that', 'the', 'row', 'must', '...</td>\n",
       "      <td>['mean', 'row', 'must', 'form', 'set', 'orthon...</td>\n",
       "      <td>mean row must form set orthonorm vector</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7.37</td>\n",
       "      <td>8.50</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7</td>\n",
       "      <td>it  mean  that  the  row  must  form  a  set  ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179470</th>\n",
       "      <td>318026</td>\n",
       "      <td>No.\\nExample:\\nA = \\n[,]\\n[,]\\nis not invertib...</td>\n",
       "      <td>#Transformations</td>\n",
       "      <td>2</td>\n",
       "      <td>15107</td>\n",
       "      <td>['No', 'Example', 'A', 'is', 'not', 'invertibl...</td>\n",
       "      <td>['no', 'exampl', 'a', 'is', 'not', 'invert', '...</td>\n",
       "      <td>['exampl', 'invert', 'still', 'diagonaliz', 'm...</td>\n",
       "      <td>exampl invert still diagonaliz matrix two eige...</td>\n",
       "      <td>transformationapplication</td>\n",
       "      <td>...</td>\n",
       "      <td>14.3</td>\n",
       "      <td>14.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.4</td>\n",
       "      <td>17.35</td>\n",
       "      <td>15.26</td>\n",
       "      <td>4.5</td>\n",
       "      <td>7</td>\n",
       "      <td>no  exampl  a  is  not  invert  but  still  di...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179720</th>\n",
       "      <td>318626</td>\n",
       "      <td>see picture :( didn't know this one was for pa...</td>\n",
       "      <td>#PhaseTransitions</td>\n",
       "      <td>4</td>\n",
       "      <td>15125</td>\n",
       "      <td>['see', 'picture', \"didn't\", 'know', 'this', '...</td>\n",
       "      <td>['see', 'pictur', \"didn't\", 'know', 'this', 'o...</td>\n",
       "      <td>['see', 'pictur', 'know', 'one', 'past']</td>\n",
       "      <td>see pictur know one past</td>\n",
       "      <td>phasetransitions</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.48</td>\n",
       "      <td>7.04</td>\n",
       "      <td>1.5</td>\n",
       "      <td>5</td>\n",
       "      <td>see  pictur  \"didnt\"  know  this  one  was  fo...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179744</th>\n",
       "      <td>318627</td>\n",
       "      <td>The isotherm most likely has a kink at</td>\n",
       "      <td>#PhaseTransitions</td>\n",
       "      <td>2</td>\n",
       "      <td>15126</td>\n",
       "      <td>['The', 'isotherm', 'most', 'likely', 'has', '...</td>\n",
       "      <td>['the', 'isotherm', 'most', 'like', 'has', 'a'...</td>\n",
       "      <td>['isotherm', 'like', 'kink']</td>\n",
       "      <td>isotherm like kink</td>\n",
       "      <td>phasetransitions</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.7</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.35</td>\n",
       "      <td>14.31</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>the  isotherm  most  like  has  a  kink  at</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>839 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Polls ID                            Poll Responses Response  \\\n",
       "225       108779        Threat for two reasons ) explanatory power.   \n",
       "1026       92219  Multiply by exp and then I'd integrate over th...   \n",
       "1027       92219  In general, I would treat θ as a nuisance para...   \n",
       "1144       99333                          beta, gamma and rate (nu)   \n",
       "1149       99335        I'm still confused about the last activity.   \n",
       "...          ...                                                ...   \n",
       "179327    305203                The second one hold when we draw ou   \n",
       "179390    310232  It means that the rows must form a set of orth...   \n",
       "179470    318026  No.\\nExample:\\nA = \\n[,]\\n[,]\\nis not invertib...   \n",
       "179720    318626  see picture :( didn't know this one was for pa...   \n",
       "179744    318627             The isotherm most likely has a kink at   \n",
       "\n",
       "       Assessment reports Hashtag  Assessment reports Score  time_stamp  \\\n",
       "225                  #objmorality                         4        4028   \n",
       "1026              #exactinference                         2        2207   \n",
       "1027              #exactinference                         2        2207   \n",
       "1144              #exactinference                         1        3141   \n",
       "1149              #exactinference                         1        3142   \n",
       "...                           ...                       ...         ...   \n",
       "179327           #Transformations                         1       14433   \n",
       "179390           #Transformations                         2       14630   \n",
       "179470           #Transformations                         2       15107   \n",
       "179720          #PhaseTransitions                         4       15125   \n",
       "179744          #PhaseTransitions                         2       15126   \n",
       "\n",
       "                                      tokenized_responses  \\\n",
       "225     ['Threat', 'for', 'two', 'reasons', 'explanato...   \n",
       "1026    ['Multiply', 'by', 'exp', 'and', 'then', \"I'd\"...   \n",
       "1027    ['In', 'general', 'I', 'would', 'treat', 'θ', ...   \n",
       "1144               ['beta', 'gamma', 'and', 'rate', 'nu']   \n",
       "1149    [\"I'm\", 'still', 'confused', 'about', 'the', '...   \n",
       "...                                                   ...   \n",
       "179327  ['The', 'second', 'one', 'hold', 'when', 'we',...   \n",
       "179390  ['It', 'means', 'that', 'the', 'rows', 'must',...   \n",
       "179470  ['No', 'Example', 'A', 'is', 'not', 'invertibl...   \n",
       "179720  ['see', 'picture', \"didn't\", 'know', 'this', '...   \n",
       "179744  ['The', 'isotherm', 'most', 'likely', 'has', '...   \n",
       "\n",
       "                                        stemmed_responses  \\\n",
       "225     ['threat', 'for', 'two', 'reason', 'explanator...   \n",
       "1026    ['multipli', 'by', 'exp', 'and', 'then', \"i'd\"...   \n",
       "1027    ['in', 'general', 'i', 'would', 'treat', 'θ', ...   \n",
       "1144               ['beta', 'gamma', 'and', 'rate', 'nu']   \n",
       "1149    [\"i'm\", 'still', 'confus', 'about', 'the', 'la...   \n",
       "...                                                   ...   \n",
       "179327  ['the', 'second', 'one', 'hold', 'when', 'we',...   \n",
       "179390  ['it', 'mean', 'that', 'the', 'row', 'must', '...   \n",
       "179470  ['no', 'exampl', 'a', 'is', 'not', 'invert', '...   \n",
       "179720  ['see', 'pictur', \"didn't\", 'know', 'this', 'o...   \n",
       "179744  ['the', 'isotherm', 'most', 'like', 'has', 'a'...   \n",
       "\n",
       "                                          clean_responses  \\\n",
       "225     ['threat', 'two', 'reason', 'explanatori', 'po...   \n",
       "1026    ['multipli', 'exp', \"i'd\", 'integr', 'theta', ...   \n",
       "1027    ['general', 'treat', 'θ', 'nuisanc', 'paramet'...   \n",
       "1144                      ['beta', 'gamma', 'rate', 'nu']   \n",
       "1149          [\"i'm\", 'still', 'confus', 'last', 'activ']   \n",
       "...                                                   ...   \n",
       "179327            ['second', 'one', 'hold', 'draw', 'ou']   \n",
       "179390  ['mean', 'row', 'must', 'form', 'set', 'orthon...   \n",
       "179470  ['exampl', 'invert', 'still', 'diagonaliz', 'm...   \n",
       "179720           ['see', 'pictur', 'know', 'one', 'past']   \n",
       "179744                       ['isotherm', 'like', 'kink']   \n",
       "\n",
       "                                                   string  \\\n",
       "225                   threat two reason explanatori power   \n",
       "1026                 multipli exp i'd integr theta margin   \n",
       "1027           general treat θ nuisanc paramet sum integr   \n",
       "1144                                   beta gamma rate nu   \n",
       "1149                          i'm still confus last activ   \n",
       "...                                                   ...   \n",
       "179327                            second one hold draw ou   \n",
       "179390            mean row must form set orthonorm vector   \n",
       "179470  exampl invert still diagonaliz matrix two eige...   \n",
       "179720                           see pictur know one past   \n",
       "179744                                 isotherm like kink   \n",
       "\n",
       "                         LOs/ HCs  ... flesch_kincaid_grade gunning_fog  \\\n",
       "225                   objmorality  ...                  7.6       10.00   \n",
       "1026            decisioninference  ...                  8.0       15.73   \n",
       "1027            decisioninference  ...                  9.6        8.51   \n",
       "1144            decisioninference  ...                  3.7        1.60   \n",
       "1149            decisioninference  ...                  2.9        2.00   \n",
       "...                           ...  ...                  ...         ...   \n",
       "179327  transformationapplication  ...                  0.5        2.00   \n",
       "179390  transformationapplication  ...                  2.5        2.80   \n",
       "179470  transformationapplication  ...                 14.3       14.23   \n",
       "179720           phasetransitions  ...                  0.5        2.00   \n",
       "179744           phasetransitions  ...                 -2.7        1.20   \n",
       "\n",
       "        smog_index  automated_readability_index  coleman_liau_index  \\\n",
       "225            0.0                         10.3               14.24   \n",
       "1026           0.0                          5.9                8.17   \n",
       "1027           0.0                          6.3                9.87   \n",
       "1144           0.0                         -1.9               -1.46   \n",
       "1149           0.0                          2.7                3.80   \n",
       "...            ...                          ...                 ...   \n",
       "179327         0.0                         -1.1                0.32   \n",
       "179390         0.0                          4.3                7.37   \n",
       "179470         0.0                         12.4               17.35   \n",
       "179720         0.0                         -0.2                1.48   \n",
       "179744         0.0                          5.2                5.35   \n",
       "\n",
       "        dale_chall_readability_score  linsear_write_formula  words_count  \\\n",
       "225                            10.20                    2.5            5   \n",
       "1026                           17.09                    4.0            7   \n",
       "1027                           13.01                    4.5            7   \n",
       "1144                           15.68                    1.0            4   \n",
       "1149                           10.20                    1.5            6   \n",
       "...                              ...                    ...          ...   \n",
       "179327                          7.04                    1.5            5   \n",
       "179390                          8.50                    2.5            7   \n",
       "179470                         15.26                    4.5            7   \n",
       "179720                          7.04                    1.5            5   \n",
       "179744                         14.31                    0.5            3   \n",
       "\n",
       "                                            lex_data_prep  Dugast  \n",
       "225          threat  for  two  reason  explanatori  power   False  \n",
       "1026    multipli  by  exp  and  then  \"id\"  integr  ov...   False  \n",
       "1027    in  general  i  would  treat  θ  as  a  nuisan...   False  \n",
       "1144                           beta  gamma  and  rate  nu   False  \n",
       "1149         \"im\"  still  confus  about  the  last  activ   False  \n",
       "...                                                   ...     ...  \n",
       "179327         the  second  one  hold  when  we  draw  ou   False  \n",
       "179390  it  mean  that  the  row  must  form  a  set  ...   False  \n",
       "179470  no  exampl  a  is  not  invert  but  still  di...   False  \n",
       "179720  see  pictur  \"didnt\"  know  this  one  was  fo...   False  \n",
       "179744        the  isotherm  most  like  has  a  kink  at   False  \n",
       "\n",
       "[839 rows x 23 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not yet put in lex.Dugast, lex.msttr(segment_window=0), lex.mattr(window_size=0),lex.hdd(draws=0),lex.Summer (math domain error)\n",
    "def generate_lexcial_richness(text):\n",
    "    lex = LexicalRichness(text)\n",
    "    try:\n",
    "        return lex.Dugast\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df['Dugast'] = df['lex_data_prep'].apply(generate_lexcial_richness)\n",
    "df.head()\n",
    "df[df['Dugast'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "839"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['Dugast'] == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polls ID</th>\n",
       "      <th>Poll Responses Response</th>\n",
       "      <th>Assessment reports Hashtag</th>\n",
       "      <th>Assessment reports Score</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>tokenized_responses</th>\n",
       "      <th>stemmed_responses</th>\n",
       "      <th>clean_responses</th>\n",
       "      <th>string</th>\n",
       "      <th>LOs/ HCs</th>\n",
       "      <th>...</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>words_count</th>\n",
       "      <th>lex_data_prep</th>\n",
       "      <th>Dugast</th>\n",
       "      <th>msttr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12522</td>\n",
       "      <td>The strengths of Plato's approach is his const...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['The', 'strengths', 'of', \"Plato's\", 'approac...</td>\n",
       "      <td>['the', 'strength', 'of', 'plato', 'approach',...</td>\n",
       "      <td>['strength', 'plato', 'approach', 'construct',...</td>\n",
       "      <td>strength plato approach construct whole framew...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>9.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>18.28</td>\n",
       "      <td>14.04</td>\n",
       "      <td>12.0</td>\n",
       "      <td>24</td>\n",
       "      <td>the  strength  of  plato  approach  is  his  c...</td>\n",
       "      <td>90.603405</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12522</td>\n",
       "      <td>In the breakout we discussed if outside the ca...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>['In', 'the', 'breakout', 'we', 'discussed', '...</td>\n",
       "      <td>['in', 'the', 'breakout', 'we', 'discuss', 'if...</td>\n",
       "      <td>['breakout', 'discuss', 'outsid', 'cave', 'big...</td>\n",
       "      <td>breakout discuss outsid cave bigger cave thus ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>11.60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>12.48</td>\n",
       "      <td>13.09</td>\n",
       "      <td>13.5</td>\n",
       "      <td>25</td>\n",
       "      <td>in  the  breakout  we  discuss  if  outsid  th...</td>\n",
       "      <td>56.677811</td>\n",
       "      <td>0.825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12522</td>\n",
       "      <td>Back to cmmon confusion time: the section 'und...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Back', 'to', 'cmmon', 'confusion', 'time', '...</td>\n",
       "      <td>['back', 'to', 'cmmon', 'confus', 'time', 'the...</td>\n",
       "      <td>['back', 'cmmon', 'confus', 'time', 'section',...</td>\n",
       "      <td>back cmmon confus time section understand inte...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>11.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>17.58</td>\n",
       "      <td>13.38</td>\n",
       "      <td>14.0</td>\n",
       "      <td>24</td>\n",
       "      <td>back  to  cmmon  confus  time  the  section  u...</td>\n",
       "      <td>47.879322</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12522</td>\n",
       "      <td>Most difficult weakness is that his position w...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Most', 'difficult', 'weakness', 'is', 'that'...</td>\n",
       "      <td>['most', 'difficult', 'weak', 'is', 'that', 'h...</td>\n",
       "      <td>['difficult', 'weak', 'posit', 'understand', '...</td>\n",
       "      <td>difficult weak posit understand testabl like i...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>11.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.4</td>\n",
       "      <td>14.85</td>\n",
       "      <td>12.84</td>\n",
       "      <td>11.5</td>\n",
       "      <td>17</td>\n",
       "      <td>most  difficult  weak  is  that  his  posit  w...</td>\n",
       "      <td>80.343345</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12522</td>\n",
       "      <td>I'm still trying to understand the significanc...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I'm\", 'still', 'trying', 'to', 'understand',...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'to', 'understand', 't...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'understand', 'signifi...</td>\n",
       "      <td>i'm still tri understand signific cave analog ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>10.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>12.53</td>\n",
       "      <td>10.69</td>\n",
       "      <td>12.5</td>\n",
       "      <td>22</td>\n",
       "      <td>\"im\"  still  tri  to  understand  the  signifi...</td>\n",
       "      <td>87.158819</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polls ID                            Poll Responses Response  \\\n",
       "0     12522  The strengths of Plato's approach is his const...   \n",
       "1     12522  In the breakout we discussed if outside the ca...   \n",
       "2     12522  Back to cmmon confusion time: the section 'und...   \n",
       "3     12522  Most difficult weakness is that his position w...   \n",
       "4     12522  I'm still trying to understand the significanc...   \n",
       "\n",
       "  Assessment reports Hashtag  Assessment reports Score  time_stamp  \\\n",
       "0         #objectivemorality                         2           1   \n",
       "1         #objectivemorality                         3           1   \n",
       "2         #objectivemorality                         2           1   \n",
       "3         #objectivemorality                         2           1   \n",
       "4         #objectivemorality                         2           1   \n",
       "\n",
       "                                 tokenized_responses  \\\n",
       "0  ['The', 'strengths', 'of', \"Plato's\", 'approac...   \n",
       "1  ['In', 'the', 'breakout', 'we', 'discussed', '...   \n",
       "2  ['Back', 'to', 'cmmon', 'confusion', 'time', '...   \n",
       "3  ['Most', 'difficult', 'weakness', 'is', 'that'...   \n",
       "4  [\"I'm\", 'still', 'trying', 'to', 'understand',...   \n",
       "\n",
       "                                   stemmed_responses  \\\n",
       "0  ['the', 'strength', 'of', 'plato', 'approach',...   \n",
       "1  ['in', 'the', 'breakout', 'we', 'discuss', 'if...   \n",
       "2  ['back', 'to', 'cmmon', 'confus', 'time', 'the...   \n",
       "3  ['most', 'difficult', 'weak', 'is', 'that', 'h...   \n",
       "4  [\"i'm\", 'still', 'tri', 'to', 'understand', 't...   \n",
       "\n",
       "                                     clean_responses  \\\n",
       "0  ['strength', 'plato', 'approach', 'construct',...   \n",
       "1  ['breakout', 'discuss', 'outsid', 'cave', 'big...   \n",
       "2  ['back', 'cmmon', 'confus', 'time', 'section',...   \n",
       "3  ['difficult', 'weak', 'posit', 'understand', '...   \n",
       "4  [\"i'm\", 'still', 'tri', 'understand', 'signifi...   \n",
       "\n",
       "                                              string     LOs/ HCs  ...  \\\n",
       "0  strength plato approach construct whole framew...  objmorality  ...   \n",
       "1  breakout discuss outsid cave bigger cave thus ...  objmorality  ...   \n",
       "2  back cmmon confus time section understand inte...  objmorality  ...   \n",
       "3  difficult weak posit understand testabl like i...  objmorality  ...   \n",
       "4  i'm still tri understand signific cave analog ...  objmorality  ...   \n",
       "\n",
       "  gunning_fog smog_index  automated_readability_index  coleman_liau_index  \\\n",
       "0        9.60        0.0                         19.2               18.28   \n",
       "1       11.60        0.0                         15.0               12.48   \n",
       "2       11.27        0.0                         18.6               17.58   \n",
       "3       11.51        0.0                         13.4               14.85   \n",
       "4       10.30        0.0                         13.5               12.53   \n",
       "\n",
       "   dale_chall_readability_score  linsear_write_formula  words_count  \\\n",
       "0                         14.04                   12.0           24   \n",
       "1                         13.09                   13.5           25   \n",
       "2                         13.38                   14.0           24   \n",
       "3                         12.84                   11.5           17   \n",
       "4                         10.69                   12.5           22   \n",
       "\n",
       "                                       lex_data_prep     Dugast  msttr  \n",
       "0  the  strength  of  plato  approach  is  his  c...  90.603405  0.925  \n",
       "1  in  the  breakout  we  discuss  if  outsid  th...  56.677811  0.825  \n",
       "2  back  to  cmmon  confus  time  the  section  u...  47.879322   0.85  \n",
       "3  most  difficult  weak  is  that  his  posit  w...  80.343345   0.85  \n",
       "4  \"im\"  still  tri  to  understand  the  signifi...  87.158819   0.95  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not yet put in lex.Dugast, lex.msttr(segment_window=0), lex.mattr(window_size=0),lex.hdd(draws=0),lex.Summer (math domain error)\n",
    "def generate_lexcial_richness(text):\n",
    "    lex = LexicalRichness(text)\n",
    "    try:\n",
    "        return lex.msttr(segment_window = 20)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df['msttr'] = df['lex_data_prep'].apply(generate_lexcial_richness)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013901573942813598"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['msttr'] == False])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.4810e+03, 1.8000e+01, 7.6000e+01, 5.4000e+01, 1.9800e+02,\n",
       "        8.4300e+02, 1.7750e+03, 6.4620e+03, 7.3612e+04, 9.2950e+04]),\n",
       " array([0.0, 0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6000000000000001,\n",
       "        0.7000000000000001, 0.8, 0.9, 1.0], dtype=object),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(df['msttr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.5000e+01, 1.2500e+02, 4.2500e+02, 8.6800e+02, 1.9980e+03,\n",
       "        5.8840e+03, 2.1039e+04, 5.4394e+04, 6.7989e+04, 2.5702e+04]),\n",
       " array([0.0483871 , 0.14354839, 0.23870968, 0.33387097, 0.42903226,\n",
       "        0.52419355, 0.61935484, 0.71451613, 0.80967742, 0.90483871,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['ttr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Polls ID</th>\n",
       "      <th>Poll Responses Response</th>\n",
       "      <th>Assessment reports Hashtag</th>\n",
       "      <th>Assessment reports Score</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>tokenized_responses</th>\n",
       "      <th>stemmed_responses</th>\n",
       "      <th>clean_responses</th>\n",
       "      <th>string</th>\n",
       "      <th>LOs/ HCs</th>\n",
       "      <th>...</th>\n",
       "      <th>lex_data_prep</th>\n",
       "      <th>Dugast</th>\n",
       "      <th>msttr</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>ttr</th>\n",
       "      <th>rttr</th>\n",
       "      <th>cttr</th>\n",
       "      <th>mtld</th>\n",
       "      <th>herdan</th>\n",
       "      <th>maas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12522</td>\n",
       "      <td>The strengths of Plato's approach is his const...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['The', 'strengths', 'of', \"Plato's\", 'approac...</td>\n",
       "      <td>['the', 'strength', 'of', 'plato', 'approach',...</td>\n",
       "      <td>['strength', 'plato', 'approach', 'construct',...</td>\n",
       "      <td>strength plato approach construct whole framew...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>the  strength  of  plato  approach  is  his  c...</td>\n",
       "      <td>90.603405</td>\n",
       "      <td>0.925</td>\n",
       "      <td>23</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>4.694855</td>\n",
       "      <td>3.319764</td>\n",
       "      <td>161.280</td>\n",
       "      <td>0.986608</td>\n",
       "      <td>0.004214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12522</td>\n",
       "      <td>In the breakout we discussed if outside the ca...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>['In', 'the', 'breakout', 'we', 'discussed', '...</td>\n",
       "      <td>['in', 'the', 'breakout', 'we', 'discuss', 'if...</td>\n",
       "      <td>['breakout', 'discuss', 'outsid', 'cave', 'big...</td>\n",
       "      <td>breakout discuss outsid cave bigger cave thus ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>in  the  breakout  we  discuss  if  outsid  th...</td>\n",
       "      <td>56.677811</td>\n",
       "      <td>0.825</td>\n",
       "      <td>23</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>3.252691</td>\n",
       "      <td>87.500</td>\n",
       "      <td>0.974096</td>\n",
       "      <td>0.008048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12522</td>\n",
       "      <td>Back to cmmon confusion time: the section 'und...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Back', 'to', 'cmmon', 'confusion', 'time', '...</td>\n",
       "      <td>['back', 'to', 'cmmon', 'confus', 'time', 'the...</td>\n",
       "      <td>['back', 'cmmon', 'confus', 'time', 'section',...</td>\n",
       "      <td>back cmmon confus time section understand inte...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>back  to  cmmon  confus  time  the  section  u...</td>\n",
       "      <td>47.879322</td>\n",
       "      <td>0.85</td>\n",
       "      <td>19</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>3.878359</td>\n",
       "      <td>2.742414</td>\n",
       "      <td>28.128</td>\n",
       "      <td>0.926491</td>\n",
       "      <td>0.023130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12522</td>\n",
       "      <td>Most difficult weakness is that his position w...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>['Most', 'difficult', 'weakness', 'is', 'that'...</td>\n",
       "      <td>['most', 'difficult', 'weak', 'is', 'that', 'h...</td>\n",
       "      <td>['difficult', 'weak', 'posit', 'understand', '...</td>\n",
       "      <td>difficult weak posit understand testabl like i...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>most  difficult  weak  is  that  his  posit  w...</td>\n",
       "      <td>80.343345</td>\n",
       "      <td>0.85</td>\n",
       "      <td>17</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.123106</td>\n",
       "      <td>2.915476</td>\n",
       "      <td>17.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12522</td>\n",
       "      <td>I'm still trying to understand the significanc...</td>\n",
       "      <td>#objectivemorality</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[\"I'm\", 'still', 'trying', 'to', 'understand',...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'to', 'understand', 't...</td>\n",
       "      <td>[\"i'm\", 'still', 'tri', 'understand', 'signifi...</td>\n",
       "      <td>i'm still tri understand signific cave analog ...</td>\n",
       "      <td>objmorality</td>\n",
       "      <td>...</td>\n",
       "      <td>\"im\"  still  tri  to  understand  the  signifi...</td>\n",
       "      <td>87.158819</td>\n",
       "      <td>0.95</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.690416</td>\n",
       "      <td>3.316625</td>\n",
       "      <td>22.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Polls ID                            Poll Responses Response  \\\n",
       "0     12522  The strengths of Plato's approach is his const...   \n",
       "1     12522  In the breakout we discussed if outside the ca...   \n",
       "2     12522  Back to cmmon confusion time: the section 'und...   \n",
       "3     12522  Most difficult weakness is that his position w...   \n",
       "4     12522  I'm still trying to understand the significanc...   \n",
       "\n",
       "  Assessment reports Hashtag  Assessment reports Score  time_stamp  \\\n",
       "0         #objectivemorality                         2           1   \n",
       "1         #objectivemorality                         3           1   \n",
       "2         #objectivemorality                         2           1   \n",
       "3         #objectivemorality                         2           1   \n",
       "4         #objectivemorality                         2           1   \n",
       "\n",
       "                                 tokenized_responses  \\\n",
       "0  ['The', 'strengths', 'of', \"Plato's\", 'approac...   \n",
       "1  ['In', 'the', 'breakout', 'we', 'discussed', '...   \n",
       "2  ['Back', 'to', 'cmmon', 'confusion', 'time', '...   \n",
       "3  ['Most', 'difficult', 'weakness', 'is', 'that'...   \n",
       "4  [\"I'm\", 'still', 'trying', 'to', 'understand',...   \n",
       "\n",
       "                                   stemmed_responses  \\\n",
       "0  ['the', 'strength', 'of', 'plato', 'approach',...   \n",
       "1  ['in', 'the', 'breakout', 'we', 'discuss', 'if...   \n",
       "2  ['back', 'to', 'cmmon', 'confus', 'time', 'the...   \n",
       "3  ['most', 'difficult', 'weak', 'is', 'that', 'h...   \n",
       "4  [\"i'm\", 'still', 'tri', 'to', 'understand', 't...   \n",
       "\n",
       "                                     clean_responses  \\\n",
       "0  ['strength', 'plato', 'approach', 'construct',...   \n",
       "1  ['breakout', 'discuss', 'outsid', 'cave', 'big...   \n",
       "2  ['back', 'cmmon', 'confus', 'time', 'section',...   \n",
       "3  ['difficult', 'weak', 'posit', 'understand', '...   \n",
       "4  [\"i'm\", 'still', 'tri', 'understand', 'signifi...   \n",
       "\n",
       "                                              string     LOs/ HCs  ...  \\\n",
       "0  strength plato approach construct whole framew...  objmorality  ...   \n",
       "1  breakout discuss outsid cave bigger cave thus ...  objmorality  ...   \n",
       "2  back cmmon confus time section understand inte...  objmorality  ...   \n",
       "3  difficult weak posit understand testabl like i...  objmorality  ...   \n",
       "4  i'm still tri understand signific cave analog ...  objmorality  ...   \n",
       "\n",
       "                                       lex_data_prep     Dugast  msttr  \\\n",
       "0  the  strength  of  plato  approach  is  his  c...  90.603405  0.925   \n",
       "1  in  the  breakout  we  discuss  if  outsid  th...  56.677811  0.825   \n",
       "2  back  to  cmmon  confus  time  the  section  u...  47.879322   0.85   \n",
       "3  most  difficult  weak  is  that  his  posit  w...  80.343345   0.85   \n",
       "4  \"im\"  still  tri  to  understand  the  signifi...  87.158819   0.95   \n",
       "\n",
       "   unique_words       ttr      rttr      cttr     mtld    herdan      maas  \n",
       "0            23  0.958333  4.694855  3.319764  161.280  0.986608  0.004214  \n",
       "1            23  0.920000  4.600000  3.252691   87.500  0.974096  0.008048  \n",
       "2            19  0.791667  3.878359  2.742414   28.128  0.926491  0.023130  \n",
       "3            17  1.000000  4.123106  2.915476   17.000  1.000000  0.000000  \n",
       "4            22  1.000000  4.690416  3.316625   22.000  1.000000  0.000000  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#not yet put in lex.Dugast, lex.msttr(segment_window=0), lex.mattr(window_size=0),lex.hdd(draws=0),lex.Summer (math domain error)\n",
    "def generate_lexcial_richness(text):\n",
    "    lex = LexicalRichness(text)\n",
    "    return lex.terms, lex.ttr, lex.rttr, lex.cttr, lex.mtld(threshold=0.72),  lex.Herdan,  lex.Maas\n",
    "\n",
    "df['unique_words'], df['ttr'], df['rttr'], df['cttr'], df['mtld'], df['herdan'], df['maas']= zip(*df['string'].apply(generate_lexcial_richness))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LexicalRichness' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0h/xyv81g2n7sj6zr0c9cw30gkc0000gn/T/ipykernel_20614/1460065545.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# instantiate new text object (use the tokenizer=blobber argument to use the textblob tokenizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLexicalRichness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Return word count.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LexicalRichness' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Measure of textual lexical diversity, computed as the mean length of sequential words in\n",
    "                a text that maintains a minimum threshold TTR score.\n",
    "\n",
    "                Iterates over words until TTR scores falls below a threshold, then increase factor\n",
    "                counter by 1 and start over. McCarthy and Jarvis (2010, pg. 385) recommends a factor\n",
    "                threshold in the range of [0.660, 0.750].\n",
    "                (McCarthy 2005, McCarthy and Jarvis 2010)\"\"\"\n",
    "\n",
    "# instantiate new text object (use the tokenizer=blobber argument to use the textblob tokenizer)\n",
    "lex = LexicalRichness(text)\n",
    "\n",
    "# Return word count.\n",
    "lex.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return (unique) word count.\n",
    "lex.terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9b5a0d0397a965b8a61bea250a52cf791a7dd32e6dbdab8d82b426f9cd3b168"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
